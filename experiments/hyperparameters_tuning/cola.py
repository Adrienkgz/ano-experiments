# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qAV4U_m8JF3F-icpID2WdjBg1nOfSckE
"""
import os
import sys
# Add the parent directory to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Désactive XLA/TPU pour ne pas interférer avec CUDA
os.environ["PJRT_DEVICE"] = "CUDA"
os.environ["XLA_USE_BF16"] = "0"
os.environ["XLA_DEVICE"] = "CUDA"
os.environ['TOKENIZERS_PARALLELISM'] = 'true'

import torch
import time
import random
import numpy as np
from datasets import load_dataset, get_dataset_split_names, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding # Assurez-vous que AdamW est bien ce que vous voulez
from torch.utils.data import DataLoader
from tqdm import tqdm
import os


torch.backends.cudnn.benchmark = True
# Imports pour la précision mixte automatique (AMP)
from torch.amp import autocast
from torch.amp import GradScaler # GradScaler reste ici car spécifique à CUDA
import os

os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

# J'ajoute des modifications et des nouvelles fonctions ci-dessous.

def set_seed(seed=42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.use_deterministic_algorithms(True, warn_only=True)

def preprocess_data(tokenizer, dataset_dict, max_length=128, num_proc=None): # Ajout de num_proc pour .map()
    """Tokenize et formate le dataset."""
    def tokenize_examples(examples):
        # padding="max_length" ici signifie que tous les exemples sont paddés à max_length globalement.
        # DataCollatorWithPadding s'assurera ensuite de la collation correcte.
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_length)

    print(f"Tokenization des données (num_proc={num_proc if num_proc else 'auto'})...")
    tokenized_datasets = dataset_dict.map(tokenize_examples, batched=True, num_proc=num_proc)

    if "label" in tokenized_datasets["train"].column_names:
        tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

    tokenized_datasets.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    return tokenized_datasets

# --- Fonctions d'entraînement et d'évaluation MODIFIÉES ---

def train_epoch(model, dataloader, optimizer, device): # Renommé train en train_epoch
    """Effectue une époque d'entraînement."""
    model.train()
    total_loss, total_correct, total_samples = 0, 0, 0
    for batch in tqdm(dataloader, desc="Training"):
        # Déplacer le batch entier vers le device
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss
        logits = outputs.logits

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=-1)
        total_correct += (preds == batch["labels"]).sum().item()
        total_samples += batch["labels"].size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = total_correct / total_samples
    return avg_loss, accuracy

@torch.no_grad()
def evaluate_model(model, dataloader, device): # Renommé evaluate en evaluate_model
    """Évalue le modèle."""
    model.eval()
    total_loss, total_correct, total_samples = 0, 0, 0
    for batch in dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        logits = outputs.logits

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=-1)
        total_correct += (preds == batch["labels"]).sum().item()
        total_samples += batch["labels"].size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = total_correct / total_samples
    return avg_loss, accuracy

import os
import json



from pathlib import Path
import time, math, random
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset, concatenate_datasets, DatasetDict, interleave_datasets
from transformers import (
    BertConfig,
    AutoTokenizer,
    BertForMaskedLM,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    get_scheduler
)
from scipy.stats import pearsonr
from tqdm.auto import tqdm, trange
from utils import save_logs

import evaluate

GLUE_TASKS_INFO = {
    "sst2": {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "cola": {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "mrpc": {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "stsb": {"num_labels": 1, "is_regression": True,  "eval_split": "validation"},
    "qqp":  {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "mnli": {"num_labels": 3, "is_regression": False, "eval_split": "validation_matched"},
    "qnli": {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "rte":  {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
    "wnli": {"num_labels": 2, "is_regression": False, "eval_split": "validation"},
}


GLUE_EVAL_SPLITS = {
    "sst2": "validation", "cola": "validation", "mrpc": "validation",
    "stsb": "validation", "qqp": "validation", "qnli": "validation",
    "rte": "validation", "wnli": "validation",
    "mnli": ["validation_matched", "validation_mismatched"]
}

INPUT_COLUMNS = {
    "sst2": ("sentence", None),
    "cola": ("sentence", None),
    "mrpc": ("sentence1", "sentence2"),
    "stsb": ("sentence1", "sentence2"),
    "qqp":  ("question1", "question2"),
    "mnli": ("premise", "hypothesis"),
    "qnli": ("question", "sentence"),
    "rte":  ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

def preprocess_glue(tokenizer, raw_ds, task):
    """Tokenise + keep labels as expected by transformers."""
    col1, col2 = INPUT_COLUMNS[task]

    def tok_fn(batch):
        if col2 is None:
            enc = tokenizer(batch[col1], truncation=True, max_length=128)
        else:
            enc = tokenizer(batch[col1], batch[col2], truncation=True, max_length=128)
        # copy label ➜ labels
        enc["labels"] = batch["label"]
        return enc

    keep_cols = [c for c in raw_ds["train"].column_names if c == "label"]
    rm_cols = [c for c in raw_ds["train"].column_names if c not in keep_cols]
    return raw_ds.map(tok_fn, batched=True, remove_columns=rm_cols)



@torch.no_grad()
def evaluate_model(model, dataloader, device, task_name, is_reg):
    """
    Évalue le modèle, retourne la perte, l'agrégat officiel, son nom, et le dict. de métriques brutes.
    """
    model.eval()
    all_preds, all_labels = [], []
    total_loss, total_samples = 0, 0
    loss_fn = torch.nn.MSELoss() if is_reg else torch.nn.CrossEntropyLoss()

    for batch in dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        labels = batch["labels"]
        outputs = model(**batch)
        logits = outputs.logits
        loss = loss_fn(logits.squeeze(-1) if is_reg else logits, labels.float() if is_reg else labels)
        total_loss += loss.item() * labels.size(0)
        total_samples += labels.size(0)
        if is_reg:
            all_preds.append(logits.squeeze(-1).cpu())
        else:
            all_preds.append(logits.argmax(dim=-1).cpu())
        all_labels.append(labels.cpu())

    avg_loss = total_loss / total_samples
    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    # Correction pour MNLI: on charge "mnli" pour les deux splits.
    metric_loader = evaluate.load("glue", "mnli" if task_name == "mnli" else task_name)
    raw_results_dict = metric_loader.compute(predictions=all_preds, references=all_labels)

    # Agrégation pour le score GLUE officiel
    if task_name == "cola":
        agg_metric, name = raw_results_dict["matthews_correlation"], "MCC"
    elif task_name == "stsb":
        agg_metric, name = (raw_results_dict["pearson"] + raw_results_dict["spearmanr"]) / 2.0, "Pearson+Spearman"
    elif task_name in ["mrpc", "qqp"]:
        agg_metric, name = (raw_results_dict["accuracy"] + raw_results_dict["f1"]) / 2.0, "Acc+F1"
    else: # Pour sst2, qnli, rte, et aussi pour chaque split de mnli
        agg_metric, name = raw_results_dict["accuracy"], "Accuracy"
    
    # Retourne l'agrégat ET le dictionnaire de métriques brutes pour un logging détaillé
    return avg_loss, agg_metric, name, raw_results_dict

def finetune_glue_task(
    task_name: str,
    seed_val: int,
    optimizer_cls: torch.optim.Optimizer,
    optimizer_name: str,
    *,
    num_epochs: int = 3,
    per_device_batch_size: int = 32,
    grad_accum_steps: int = 1,
    lr_scaling_rule: str = "linear",  # "linear" ou "none"
    warmup_ratio: float = 0.1,
    use_small_subset: bool = False,
    optimizer_params: dict[str, any] | None = None,
):
    """Fine-Tune BERT sur une tâche GLUE.

    Args:
        optimizer_cls: classe optimiseur (AdamW, Lion…)
        optimizer_kwargs: kwargs spécifiques transmis à l'optimiseur (ex. betas).
    """

    base_learning_rate = optimizer_params['lr']
    optim_params = optimizer_params.copy()
    del optim_params['lr']
    set_seed(seed_val)
    device = torch.device('cuda')

    info = GLUE_TASKS_INFO[task_name]
    print(f"\n### FINETUNE {task_name} – seed={seed_val} – optimiser={optimizer_name}")
    raw = load_dataset("glue", task_name)
    if use_small_subset:
        raw["train"] = raw["train"].shuffle(seed=seed_val).select(range(5_000))
        if task_name == "mnli":
            raw["validation_matched"] = raw["validation_matched"].shuffle(seed=seed_val).select(range(200))
            raw["validation_mismatched"] = raw["validation_mismatched"].shuffle(seed=seed_val).select(range(200))
        else:
             raw[GLUE_EVAL_SPLITS[task_name]] = raw[GLUE_EVAL_SPLITS[task_name]].shuffle(seed=seed_val).select(range(200))

    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    ds = preprocess_glue(tokenizer, raw, task_name)
    collator = DataCollatorWithPadding(tokenizer)

    train_dl = DataLoader(
        ds["train"],
        batch_size=per_device_batch_size,
        shuffle=True,
        collate_fn=collator,
    )
    if task_name == "mnli":
        eval_dl_matched = DataLoader(ds["validation_matched"], batch_size=per_device_batch_size, collate_fn=collator)
        eval_dl_mismatched = DataLoader(ds["validation_mismatched"], batch_size=per_device_batch_size, collate_fn=collator)
    else:
        eval_dl = DataLoader(ds[GLUE_EVAL_SPLITS[task_name]], batch_size=per_device_batch_size, collate_fn=collator)

    # Model initialisation
    model = AutoModelForSequenceClassification.from_pretrained(
        "bert-base-uncased", num_labels=info["num_labels"]
    ).to(device)
    
    effective_batch = per_device_batch_size * grad_accum_steps
    if lr_scaling_rule == "linear":
        learning_rate = base_learning_rate * (effective_batch / 32)
    else:
        learning_rate = base_learning_rate
        
    if 'weight_decay' not in optim_params:
        raise ValueError(
            "Optimizer parameters must include 'weight_decay' for GLUE fine-tuning."
        )

    # Optimizer initialization
    optim = optimizer_cls(
        model.parameters(), lr=learning_rate, **optim_params
    )

    # Scheduler
    num_update_steps_per_epoch = len(train_dl) // grad_accum_steps
    max_train_steps = num_update_steps_per_epoch * num_epochs
    warmup_steps = int(warmup_ratio * max_train_steps)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optim,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps,
    )

    scaler = GradScaler()
    logs: list[dict[str, float]] = []

    # Training loop
    global_step = 0
    for ep in tqdm(range(1, num_epochs + 1), desc="Epochs", unit="ép"):
        model.train()
        tot_loss, tot_samples = 0.0, 0

        for step, batch in enumerate(
            tqdm(train_dl, desc=f"Train | ép {ep}", leave=False, unit="batch")
        ):
            batch = {k: v.to(device) for k, v in batch.items()}
            with autocast(device_type="cuda"):
                loss = model(**batch).loss / grad_accum_steps 

            scaler.scale(loss).backward()

            if (step + 1) % grad_accum_steps == 0 or (step + 1) == len(train_dl):
                scaler.step(optim)
                scaler.update()
                optim.zero_grad()

                lr_scheduler.step()
                global_step += 1

            # Stats
            bsz = batch["labels"].size(0)
            tot_loss += loss.item() * grad_accum_steps * bsz
            tot_samples += bsz

        tr_loss = tot_loss / tot_samples

        # Evaluation
        if task_name == "mnli":
            loss_m, acc_m, _, raw_m = evaluate_model(model, eval_dl_matched, device, task_name, info["is_regression"])
            loss_mm, acc_mm, _, raw_mm = evaluate_model(model, eval_dl_mismatched, device, task_name, info["is_regression"])
            ev_loss = (loss_m + loss_mm) / 2.0
            ev_metric = (acc_m + acc_mm) / 2.0
            metric_name = "Accuracy-Avg(m/mm)"
            raw_metrics = {"matched": raw_m, "mismatched": raw_mm}
            tqdm.write(f"Epoch {ep}/{num_epochs} - Train {tr_loss:.4f} | Eval {ev_loss:.4f} | {metric_name} {ev_metric:.4f} (m: {acc_m:.4f}, mm: {acc_mm:.4f})")
        else:
            ev_loss, ev_metric, metric_name, raw_metrics = evaluate_model(model, eval_dl, device, task_name, info["is_regression"])
            # Affichage cohérent (valeurs brutes, 4 décimales)
            tqdm.write(f"Epoch {ep}/{num_epochs} - Train {tr_loss:.4f} | Eval {ev_loss:.4f} | {metric_name} {ev_metric:.4f}")

        # Logging enrichi avec les métriques brutes
        logs.append({
            "epoch": ep,
            "train_loss": tr_loss,
            "eval_loss": ev_loss,
            "aggregated_metric": {metric_name: float(ev_metric)},
            "raw_metrics": raw_metrics
        })
        
    # Save logs
    save_logs(
        optimizer_name,
        seed_val,
        logs,
        folder=f"experiments/hyperparameters_tuning/logs/mrpc",
    )

if __name__ == "__main__":
    import torch
    from optimizers import *
    
    #optimizers = [torch.optim.AdamW, Ano, Grams]
    lr = [7e-6, 2e-5, 7e-5]
    beta1 = [0.95, 0.97, 0.98]
    #beta2 = [0.95, 0.99, 0.999]
    optimizer_to_test = []
                    
    """for opt_class in optimizers:
        for learning_rate in lr:
            for b1 in beta1:
                for b2 in beta2:
                    opt_name = f"{opt_class.__name__}_lr{learning_rate}_b1{b1}_b2{b2}"
                    opt_params = {'lr': learning_rate, 'betas': (b1, b2), 'weight_decay': 1e-2}
                    optimizer_to_test.append((opt_class, opt_name, opt_params))"""
                    
    """beta3 = [0.9, 0.94, 0.96]
    opt_class = Adan
    for learning_rate in lr:
            for b1 in beta1:
                for b3 in beta3:
                    opt_name = f"{opt_class.__name__}_lr{learning_rate}_b1{b1}_b2{b3}"
                    opt_params = {'lr': learning_rate, 'betas': (b1, 0.92, b3), 'weight_decay': 1e-2}
                    optimizer_to_test.append((opt_class, opt_name, opt_params))"""
    
    optimizers_to_test = [(Lion,'Lion'),]
    lr = [7e-6, 2e-5, 7e-5]
    beta1 = [0.9, 0.92, 0.95]
    beta2 = [0.9, 0.99, 0.999]
    optimizers = []            
                    
    for opt_class, opt_name in optimizers_to_test:
        for learning_rate in lr:
            for b1 in beta1:
                for b2 in beta2:
                    opt_name_file = f"{opt_name}_lr{learning_rate}_b1{b1}_b2{b2}"
                    opt_params = {'lr': learning_rate, 'betas': (b1, b2), 'weight_decay': 1e-2}
                    optimizers.append((opt_class, opt_name_file, opt_params))
    
    #beta1 = [0.95, 0.97, 0.98]
    #beta3 = [0.9, 0.94, 0.96]
    #opt_class = Adan
    #for learning_rate in lr:
    #        for b1 in beta1:
    #            for b3 in beta3:
    #                opt_name = f"{opt_class.__name__}_lr{learning_rate}_b1{b1}_b2{b3}"
    #                opt_params = {'lr': learning_rate, 'betas': (b1, 0.92, b3), 'weight_decay': 1e-2}
    #                optimizers_to_run.append((opt_class, opt_name, opt_params))
    
    seeds_to_run = [2, 5, 10, 42, 444]
    for seed in seeds_to_run:
        TASKS = ['mrpc']
        for task in TASKS:
            for opt_class, opt_name, opt_params in optimizers:
            
                finetune_glue_task(
                    num_epochs=5,
                    task_name=task,
                    seed_val=seed,
                    optimizer_cls=opt_class,
                    optimizer_name=opt_name,
                    optimizer_params=opt_params,
                    use_small_subset=False,
                )